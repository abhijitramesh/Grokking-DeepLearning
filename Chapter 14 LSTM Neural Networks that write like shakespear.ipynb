{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RNN Character Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-b8d5854570fb>:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import Collection\n"
     ]
    }
   ],
   "source": [
    "import sys,random,math\n",
    "from collections import Collection\n",
    "import numpy as np\n",
    "import sys\n",
    "from Chapter_13.tensor import Tensor\n",
    "from Chapter_13.layers.layers import Embedding,RNNCell\n",
    "from Chapter_13.loss.loss import CrossEntropyLoss\n",
    "from Chapter_13.optmizers.gradient_descent import SGD\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "f = open(\"shakespear.txt\")\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "indices = np.array(list(map(lambda x:word2index[x], raw)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
    "model = RNNCell(n_input=512,n_hidden=512,n_output=len(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(model.get_parameters()+embed.get_parameters(),alpha=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bptt = 16\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[:n_batches*batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches-1) / bptt))\n",
    "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
    "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That,\n",
      "[37 50 36 21 18]\n"
     ]
    }
   ],
   "source": [
    "print(raw[0:5])\n",
    "print(indices[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37 31 19 40 46 50 37 12 12 36 36 39 46 12 18  5 12 43 50 19 39 36 12 12\n",
      "  12 60 12 60 60 12 12  9]\n",
      " [50  5  5 40  1 39 36 51 43  9 45 39 45 21 12  5 55 46 46  5 45 25 55 29\n",
      "  50  9  8  4  9  4 56 12]\n",
      " [36  5 14 39 39 36 45 10 46 12 45 19 29 50 28 49 39 51 45  5 29 39 38 39\n",
      "  39 16 46 18 12 46 46 55]\n",
      " [21 57 37 38 12 38 55 12 51  9 12 12 31 39 39 47 39 18 43 48 10 12 46 39\n",
      "  38 45 51  5 32 38  4 46]\n",
      " [18 47 60 14 28 12 46 38 12 46 28 24  5 12 12 52  9 12 12 46 31 43 28  1\n",
      "  18 36 45 47 51 39 39 43]]\n"
     ]
    }
   ],
   "source": [
    "print(batched_indices[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[37 31 19 ... 12 12  9]\n",
      "  [50  5  5 ...  4 56 12]\n",
      "  [36  5 14 ... 46 46 55]\n",
      "  ...\n",
      "  [ 9  0 39 ... 21 46 12]\n",
      "  [21 19  8 ... 50 36 60]\n",
      "  [39  5 21 ... 12 21  9]]\n",
      "\n",
      " [[ 4 48 12 ...  1 50 12]\n",
      "  [ 1 46 10 ... 36 31 50]\n",
      "  [21 28 60 ... 60  5 60]\n",
      "  ...\n",
      "  [14 46 10 ...  9  5 39]\n",
      "  [29  9 21 ... 12 47 29]\n",
      "  [12 39 12 ... 46 43 60]]\n",
      "\n",
      " [[21 12 40 ... 30 18  9]\n",
      "  [50 21 60 ... 39 12 26]\n",
      "  [46 50  9 ... 38 36 16]\n",
      "  ...\n",
      "  [12 12 38 ... 29 12 12]\n",
      "  [40 11 39 ... 12 51 60]\n",
      "  [36 12 12 ...  9  1  9]]\n",
      "\n",
      " [[60 28 60 ... 46 46 12]\n",
      "  [21 46 10 ... 21  9 39]\n",
      "  [50 51 12 ... 12 12 60]\n",
      "  ...\n",
      "  [12 12 38 ... 50 43 39]\n",
      "  [ 8 36 12 ... 46 12 12]\n",
      "  [46 12 36 ... 38 28 55]]\n",
      "\n",
      " [[ 9  4 45 ... 10 46 38]\n",
      "  [21 60 60 ... 39 38 46]\n",
      "  [38  9 25 ... 12 45 51]\n",
      "  ...\n",
      "  [21 36 32 ... 12 28 46]\n",
      "  [50 43 37 ... 40 39 28]\n",
      "  [39 12 23 ... 39 38 12]]]\n"
     ]
    }
   ],
   "source": [
    "print(input_batches[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[50  5  5 ...  4 56 12]\n",
      "  [36  5 14 ... 46 46 55]\n",
      "  [21 57 37 ... 38  4 46]\n",
      "  ...\n",
      "  [21 19  8 ... 50 36 60]\n",
      "  [39  5 21 ... 12 21  9]\n",
      "  [ 4 48 12 ...  1 50 12]]\n",
      "\n",
      " [[ 1 46 10 ... 36 31 50]\n",
      "  [21 28 60 ... 60  5 60]\n",
      "  [18 33  9 ...  9  5 10]\n",
      "  ...\n",
      "  [29  9 21 ... 12 47 29]\n",
      "  [12 39 12 ... 46 43 60]\n",
      "  [21 12 40 ... 30 18  9]]\n",
      "\n",
      " [[50 21 60 ... 39 12 26]\n",
      "  [46 50  9 ... 38 36 16]\n",
      "  [51 39 29 ... 16 43 28]\n",
      "  ...\n",
      "  [40 11 39 ... 12 51 60]\n",
      "  [36 12 12 ...  9  1  9]\n",
      "  [60 28 60 ... 46 46 12]]\n",
      "\n",
      " [[21 46 10 ... 21  9 39]\n",
      "  [50 51 12 ... 12 12 60]\n",
      "  [40 45 10 ... 10 46 26]\n",
      "  ...\n",
      "  [ 8 36 12 ... 46 12 12]\n",
      "  [46 12 36 ... 38 28 55]\n",
      "  [ 9  4 45 ... 10 46 38]]\n",
      "\n",
      " [[21 60 60 ... 39 38 46]\n",
      "  [38  9 25 ... 12 45 51]\n",
      "  [60 51 39 ... 11 29 26]\n",
      "  ...\n",
      "  [50 43 37 ... 40 39 28]\n",
      "  [39 12 23 ... 39 38 12]\n",
      "  [38 60 27 ... 36 12 21]]]\n"
     ]
    }
   ],
   "source": [
    "print(target_batches[0:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YQGQYQYYYQYQYQGQQQQQYQYQQQQYYQQYQYQYQYQYQGGQYQYQYQYQQYQGYQYQYQYQQYYQQYQQQQYQQYQYYQYYQQQYQYQGQYQYQQYQQYQQYQYQYQQQYYQQQQQQYQYQYQYQQQQYQYQGQQYQYQYQQYQYQYQQGYQGGGYQQQQQQQYQQQQYYQQQQQQYYQQYYQYQYQGYQYYQQYQQYQQYYQQYQYQYQYQGQQYQGYQYQQQYQQYYQYQYQYQYQGQYQYQYQYQQQYQGYQQQYQQYYQQQQYQQYQQYQYQYQYQQYQYQYQYQGYQQQQQYQGYQYYYYYQQQYQQQQYQQQQYQYQYQYQYQYQGGYQGQQYYQQQQQQYQQQYQYQYYYQGGYYQQYQGGQGYYQYQGGQQGQYQQYQQQYQYQYQYQQYQYQQQQYQYQQYQYQYYQQYQQYQQYQYQQQQYQYQQQYQGGYQYQYQYQQQQYQQYQGGYYQQQYQYQYQYYQYYQYQQYQQYQYQQYQGGQQYQGGGGYQYQGYYQYQQQYQYQQQYQYQGQQYQGYQYQGYQYQQGYQQYQGYQQGYQYYQYQYQYQQQQYQYQQYQQQYQQYYQQQYYQYQYQYQGYQYQYQQYQYQQYQQYQGQYYQGQYQQYQYQQYQYQYQYQGGQYQQQYQYQYQQYQGYQQQQQYQYQYQQYQYQQGYYQQQQQQYQYQGGQYQGQYQYQYQQQYQYQYQYQQGQYQYQYQYQGQYQYQYQQYQGGYQYQQQYQGGQYQYQGYQQYQQYQYYQYQGQYYYQGYQYQYQYQQQQYQQQQGQYQYQYQQYQQYQYQYQQYQQGGQQYQQQYQYQYYQYQYQGQQYQYQYQQYQYQQQYQQYQQQQQYQGGYQGQQGQQYQYQYQQQYQYQYQYQGGYQGQYQYQYQYQGGYYQYQQYYYQGYQYQGYQYQQYQYQQYQQGYQGGGYQQQYQGQQYQQQYQYYQQQQYQYQYQQQYQQGQYYQYYQYQQYQQYQYQQGYQYYQYQGGQYYQYQYQYQYQGYQYYQQQYQYQQYQGQQYQQQYQQQQYQQYQGYQYQQYQQYQQYQGYQGGYQQYQYQYQGQQQQQQYQQYQQYYQQYQYQQYQQYQYQQQYQGYQYQQQYQGGGGYQGGQYQGYQYQYQYQYQYQQQQQQYQQYQQYQYQQQQYYQYQYQYQQYQQYYYYQQQQYQQQYQYQYQGGGQYQQYQYQQQYQQQYQGQYQQQYQYQYQYYQGGQYQYQYQYQYQQYQYQGYYQQYYQQYQYQGYQQYQYQYYQQQYQYQQGYQYQYQYQYQGGYQYQYYQYQYQYQYQYQQYQQQYQGGQYYQGQQQYQYQGQQYQQQYQGYQQYQYQYQYQGQYQYQQYQYQQYQYYQGQQGQYYQQQQYQQYYQQQQYQGYQGQQYQGYQYQGYYQGYQGQQGQQYQYQQQQYYQYQQQYQQQYQGYQGGQQYQYYQYQYQQYQYQYYQGGGYQQYYQYQQQYQGQYQYQYQGQYQQGGQYQQQQQQYQGYQQYQYYQYQQQQYQQQYQQYQYQGGGGQQQQQQQYQQYQYYQYQQYQQYYQYQQYQYQYQGQQYQYQYQYQYQYQYQYQQYQQYQYQYQQQYYQYQYQQQQQYQQQYQYQQQQYQYQYQGYYQYQGQQQQYQQQQYQQQYQQGYQGYQYQQYQYQYQQYQGGQGYYYQGQQYQQYYYQQQQQYYQGYQGYQYQQYQQYQYQQQQQYQYQQQQQYQQQQQYQQQYQQYYQGQYQYQQYQGYQQQYQYQGQYYQQGQQQQQQQYQQYQYQQYQGGQYQQYQGGQQYQQYQGGYQYQGYQYQQYQGGYYYQQQYQYQYQQYQQYQQYQYYQYQYQQQQQQYQYQYQQYQYQYQGQQQQYQQQQQYQYQQQQQYQQYQGQQQQQYQQYQYQYQYQQYQYQYQQYQYQQQQYQYQQYQQGQQQQYQQYQYQYQYQGGQQGQQQYQGQQQQYQQQQQYQGYQQYQQYQYQQYQQYQYQYQQYQQYQGGYQYQYQGYQYQGQYQYQYQYQQQYQQYQQQYQGQQYQQYQY\n"
     ]
    }
   ],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "\n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "#         m = output.data.argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "print(generate_sample(n=2000, init_char='\\n'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(iterations=400):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "\n",
    "        hidden = model.init_hidden(batch_size=batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if(t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if(batch_i == 0):\n",
    "                log += \" - \" + generate_sample(n=70, init_char='\\n').replace(\"\\n\",\" \")\n",
    "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 191/195 - Loss:3.9091070364026326ThYerd the tYesY the the the Yarst to the the tYest to the the the to \n",
      " Iter:1 - Batch 191/195 - Loss:3.8390553843301007This, a the to the to the tYest to the YarsY the the to the to the to \n",
      " Iter:2 - Batch 191/195 - Loss:3.7643283883435803This, a the to the to the tYeY the Yarst to the to the tYeY the to the\n",
      " Iter:3 - Batch 191/195 - Loss:3.6691792363681364 ThY the the the the the the the the the the the tYest to the the the t\n",
      " Iter:4 - Batch 191/195 - Loss:3.6236716012921096This, a the the the the the the Yarse Yarse the to the tYest to the th\n",
      " Iter:5 - Batch 191/195 - Loss:3.5250916813977176 This, a the the Yars, a the the the the the the the the the the the th\n",
      " Iter:6 - Batch 191/195 - Loss:3.4419281899634467his, a the tY wits, Yarse the Yarse Yarse Yarse the Yarse the the the\n",
      " Iter:7 - Batch 191/195 - Loss:3.3713329852868585This, a sink the the the the to the the the the the the the the the th\n",
      " Iter:8 - Batch 191/195 - Loss:3.3396248876963783This, a sirink the the the the the the the the the the the the the the\n",
      " Iter:9 - Batch 191/195 - Loss:3.2484144441015395Thich the the the the sink the the the sink the the the the the the th\n",
      " Iter:10 - Batch 191/195 - Loss:3.1629172713630167 This, a spepirink the the the the the the the the the the the the the \n",
      " Iter:11 - Batch 191/195 - Loss:3.1411406251287996This, a speY the tarse the tarse the tarse the tYarbland himsed himsed\n",
      " Iter:12 - Batch 191/195 - Loss:3.0619888297100366This, a spepY spept the tarse the the tYarborthink the the the the the\n",
      " Iter:13 - Batch 191/195 - Loss:3.0240487593546134This, a speak.  Y would the the the the the the the the the the the th\n",
      " Iter:14 - Batch 191/195 - Loss:2.9720220698939914ThY to the tYar but to the tYar but tYar but tYar but tYar but tYar bu\n",
      " Iter:15 - Batch 191/195 - Loss:2.8322738062611024ThisY would the the the the the the the the the the the tarse the the \n",
      " Iter:16 - Batch 191/195 - Loss:2.7686150997110753 The the the the the tarst tYarbY the tarse the tarse the the the the t\n",
      " Iter:17 - Batch 191/195 - Loss:2.7677412314668345 This, a spear but the to the tYarbet, a spear but the tarse the tarse \n",
      " Iter:18 - Batch 191/195 - Loss:2.7435919160539847 Thickink the the the the the the the the the the the the the the the t\n",
      " Iter:19 - Batch 191/195 - Loss:2.6975569329466387The the the the the the the the the the the the the the the the the to\n",
      " Iter:20 - Batch 191/195 - Loss:2.6057350694907573 The tYarbarst the tolthink the tYarbarbet the tYarbarbet the tolthink \n",
      " Iter:21 - Batch 191/195 - Loss:2.6111621767772437 The tamsY YarstY the tarse the tarse the tarse the tarse the tarse the\n",
      " Iter:22 - Batch 191/195 - Loss:2.5469991950857924 Thimsed hims, a the tarse the tarse the tarse the tarse the tarse the \n",
      " Iter:23 - Batch 191/195 - Loss:2.5196460273658423Thimse the tollainter Be the tYarbarbarbarbarbarbarbarbarbarbarbarbarb\n",
      " Iter:24 - Batch 191/195 - Loss:2.4655216193728185Thim the tYarbarbarbarbarbarbarbarbarbarbarbarbarbarbarbarbarbarbarbar\n",
      " Iter:25 - Batch 191/195 - Loss:2.4299958839869977 Thisady would the to the to the to the to the to the to the to the to \n",
      " Iter:26 - Batch 191/195 - Loss:2.3212259962875597 Thimse the tolY the tols.  MAUM, a the tols.  MAUM, spearthink the tol\n",
      " Iter:27 - Batch 191/195 - Loss:2.2942007022693023 Thimsed himse the tollair, Yarbarbarbarbarbarbarbarbarbarbarbarbarbarb\n",
      " Iter:28 - Batch 191/195 - Loss:2.2608055740028417Thimsed himse to the to the to the to the to the to the to the to the \n",
      " Iter:29 - Batch 191/195 - Loss:2.2491588766157435hiYfty with to the tarse the tarse the tYarbarbarbarbarbarbarbarbarba\n",
      " Iter:30 - Batch 191/195 - Loss:2.2004465860178766ThimY For but to the to the to the to the to the to the to the to the \n",
      " Iter:31 - Batch 191/195 - Loss:2.1948373935045273Thim the toldibl the tarthink the toldibl the toldibY the toldibl the \n",
      " Iter:32 - Batch 191/195 - Loss:2.1815874806396226Thims, with tollair, the tollaiYY the tollaichis, a to the tollaiY: Th\n",
      " Iter:33 - Batch 191/195 - Loss:2.2234931218501372 To the tarse to the tarse to the tarse to the tarse to the tarse to th\n",
      " Iter:34 - Batch 191/195 - Loss:2.1996932753709153 The tarse to the tarse to the tarse to the tarse to to the tarse to th\n",
      " Iter:35 - Batch 191/195 - Loss:2.1167751693879686Thimset to the taridabs tod told toY the taridabs tY the taridabs tod \n",
      " Iter:36 - Batch 191/195 - Loss:2.0647578354862876Thim tY the tar'e the tar'e the tar'e the tar'e the tar'e the tar'e th\n",
      " Iter:37 - Batch 191/195 - Loss:2.0876372598347506Thim Yorse tY speath, a the tollair, sirst to the tollost to the tollo\n",
      " Iter:38 - Batch 191/195 - Loss:1.9845009757240917 Thimset tamset tamset tamset tamset tamset tamset tamset tamset tamset\n",
      " Iter:39 - Batch 191/195 - Loss:1.9501062102531213Thim it told told told told tolY the told told told told told told tol\n",
      " Iter:40 - Batch 191/195 - Loss:2.0348766442680093 ThiYOY won won won won won won won won won won won won won won won won\n",
      " Iter:41 - Batch 191/195 - Loss:2.0143711396735653 Thish tollow to to the toom.  Sect to the toom.  Sect to the tooY to t\n",
      " Iter:42 - Batch 191/195 - Loss:1.9194811821012965Thisbolts to to to to to to to to to to to to to to to to to to to to \n",
      " Iter:43 - Batch 191/195 - Loss:1.9101647390026173 Thick with prahe the tood told hims the tood told hims the tood told h\n",
      " Iter:44 - Batch 191/195 - Loss:1.9085952313520296 This the tanchind kind kind kind kind kind kind kind kind kind kind ki\n",
      " Iter:45 - Batch 191/195 - Loss:1.8925188941926956This to to the tolthYedind would himset tolt tolt tolthind himsY will \n",
      " Iter:46 - Batch 191/195 - Loss:1.8356986662617316 That to to the tarbet to to the tarbet to to the tarbet to to tY the t\n",
      " Iter:47 - Batch 191/195 - Loss:1.7547601359368539 ThY prit, but tollod told the tand with priuctod himset to the tand wi\n",
      " Iter:48 - Batch 191/195 - Loss:1.7107039293809911 Thick to the tamost for but tolt the tamseY himset tolt the tamset mon\n",
      " Iter:49 - Batch 191/195 - Loss:1.7556882777052119This to to the tamset told tolY the tamset told tolY tYeseYt told tolt\n",
      " Iter:50 - Batch 191/195 - Loss:1.8468840329830165 Thiridand would himset tood told the tamon with prift toY woold himset\n",
      " Iter:51 - Batch 191/195 - Loss:1.7642719062080918 Thiribl the tand himset told haYd hims the tand himset told haYd hims \n",
      " Iter:52 - Batch 191/195 - Loss:1.7526924405771158Thirt to to the tand with prink bY the tamand with pepirit, but to to \n",
      " Iter:53 - Batch 191/195 - Loss:1.7207039809687696This the tanchind himset to the tanchind himset to the namy tolt the n\n",
      " Iter:54 - Batch 191/195 - Loss:1.7824009957489182 Thim the tamand him he ktole the taYd the tamand him he ktole the tama\n",
      " Iter:55 - Batch 191/195 - Loss:1.7586053789419431 The tamYess bond would hady: apt to the parbord hastand wand hady: aY \n",
      " Iter:56 - Batch 191/195 - Loss:1.7092922001768291 Therd himset tood mon would himset tood mon would himset tood mon woul\n",
      " Iter:57 - Batch 191/195 - Loss:1.7175333021863756 Thichimsets, speakpt the namY himset to the namiour jreath prift hims \n",
      " Iter:58 - Batch 191/195 - Loss:1.6751772148521342 This the namset tood, by the would himset tood, bY the would himset to\n",
      " Iter:59 - Batch 191/195 - Loss:1.6224360826969064There iY the namy himset himset himset himset himset himset himset him\n",
      " Iter:60 - Batch 191/195 - Loss:1.5777966474659737The woY this to the namy himset to the namy himset toY would himset to\n",
      " Iter:61 - Batch 191/195 - Loss:1.5841975735600013 There the YaY so to the Yand himst Lank with pest fookt Whis to the Ya\n",
      " Iter:62 - Batch 191/195 - Loss:1.5363160404742349here it to the tamset to the tamst Lath speme, to the tamst Lath spem\n",
      " Iter:63 - Batch 191/195 - Loss:1.5330516421379914he wand himset the namy himset the namy himset the namy himset the na\n",
      " Iter:64 - Batch 191/195 - Loss:1.6170632299350337 This to to thouYd spemants fooknd would himset mon witl oure to thould\n",
      " Iter:65 - Batch 191/195 - Loss:1.5224123329936599 Therd himset tood mand with peand himset tood mand with peand himset t\n",
      " Iter:66 - Batch 191/195 - Loss:1.4567329924672867 To sommemnind YarYd hY with pesed: for but mands, speath peYt the namy\n",
      " Iter:67 - Batch 191/195 - Loss:1.5249699514676185 There it tYere it the namY the won would himse agarpon with priftY wot\n",
      " Iter:68 - Batch 191/195 - Loss:1.4286708203751047There would himse the won would himse the won would himse the won woul\n",
      " Iter:69 - Batch 191/195 - Loss:1.4234546780448756 To worset fooknd with pesYORNOSTOS: Yit won would himset to the woolts\n",
      " Iter:70 - Batch 191/195 - Loss:1.4622186059012687 There in the Yar'd himset to the speYt Yack wernish a wandsYYO: Ypon w\n",
      " Iter:71 - Batch 191/195 - Loss:1.4285335059428805 There the tamsetshish this this tY cands, but mastarbath the tamsetshi\n",
      " Iter:72 - Batch 191/195 - Loss:1.4547991566993486 ThY with ey, sYORUGHE bett YORNSINS: HiYds, apYdreYt to this to this t\n",
      " Iter:73 - Batch 191/195 - Loss:1.4896725492060834 This tamse ar'e in theY himse and himse and himse and himse and himse \n",
      " Iter:74 - Batch 191/195 - Loss:1.4565752441715336 Therd himset to the tamst Lands in the tamsY will but mastainterbett m\n",
      " Iter:75 - Batch 191/195 - Loss:1.3987075021911657 There wers, with pept this this this this this this this this this thi\n",
      " Iter:76 - Batch 191/195 - Loss:1.4040444200476965Therd himseY but mastaridget to the sperthee, and Yarbar'd have sperth\n",
      " Iter:77 - Batch 191/195 - Loss:1.3660524305931347Therd himset himst Lay, speath, a mand with pesind with pesind with pe\n",
      " Iter:78 - Batch 191/195 - Loss:1.3978305109397262 Thish this to this this to this this to this this to this this to this\n",
      " Iter:79 - Batch 191/195 - Loss:1.4453232749996154 ThisY with prift himse speath, appere the namsery YIAld.  DUDEYd so ma\n",
      " Iter:80 - Batch 191/195 - Loss:1.3386974617386724 Therd fent wand hadongeds, apt told.  MAESTOS: And cYese, Thish wonl s\n",
      " Iter:81 - Batch 191/195 - Loss:1.4674435517672453 Thish agakts, suckink but mastar.  Second some, maytimsed stod himse s\n",
      " Iter:82 - Batch 191/195 - Loss:1.4429379406485257 To stainters so these the kind Yarbase the kto these the kto this tams\n",
      " Iter:83 - Batch 191/195 - Loss:1.4041612984017242 This stodYemYed borthim stainterbe, ThisYY with prost Lath pent wond w\n",
      " Iter:84 - Batch 191/195 - Loss:1.3680488654627547 There in a faiup in a famse agagent me hYd your jreath, a this to this\n",
      " Iter:85 - Batch 191/195 - Loss:1.3278503227874272 ThY was, Sas, MadyI appease, This tampYdsYORIAX: Golt didh, Tiddingmin\n",
      " Iter:86 - Batch 191/195 - Loss:1.3362984084429899 This tamp Mas cond so by himse off YaYd himset himset himset himset hi\n",
      " Iter:87 - Batch 191/195 - Loss:1.3026482546414508 Thish at the namind himset hims this to this to this to this to this t\n",
      " Iter:88 - Batch 191/195 - Loss:1.3183124387748373 This tamse agarbe had stage, That I have the ngesere, not and sYORUTER\n",
      " Iter:89 - Batch 191/195 - Loss:1.2990857464595888 This tampet himse agants speak to this to this to this to this to this\n",
      " Iter:90 - Batch 191/195 - Loss:1.3134299685651445 There it givers, speaght Lstempind so may, spay, spay, spay, spay, spa\n",
      " Iter:91 - Batch 191/195 - Loss:1.2709877607621827Thish a mast fitthimsedY-G prow to this this tY himset himset himset h\n",
      " Iter:92 - Batch 191/195 - Loss:1.2444995425402265 Thish agalt firt find was confent maYes, speak to this to this to this\n",
      " Iter:93 - Batch 191/195 - Loss:1.3454913734165335 Thish am dYMivershimse miYldint wand with proffemn wichimse may, spard\n",
      " Iter:94 - Batch 191/195 - Loss:1.3414575435724498 There was cYeseY his gentershiold wery wery wery wery wery wery wery w\n",
      " Iter:95 - Batch 191/195 - Loss:1.3140779289732647 Thas speave himse spard most for but mand with prift himse spard most \n",
      " Iter:96 - Batch 191/195 - Loss:1.2906638550945564Thish a spar'd himse and sYORUTYI will- Thish a sparbe, Thish a sY wit\n",
      " Iter:97 - Batch 191/195 - Loss:1.2755005121453356 Thish ampet whimse Ydowst faYd sparYUnd with prift wand shatherd himse\n",
      " Iter:98 - Batch 191/195 - Loss:1.2344953259595506 Thish a YasYd spard mandssar'd himse agaY: Second so mandssar'd himseY\n",
      " Iter:99 - Batch 191/195 - Loss:1.2171109110691496 ThisYarbarbarbady: and so mandssYd sYarum day, spYost and so mand with\n",
      " Iter:100 - Batch 191/195 - Loss:1.3296306746993753 Thish a shok thy hadond wandYest and some, maytinamset himse speakesYY\n",
      " Iter:101 - Batch 191/195 - Loss:1.2266955311427912 Thish a spaint GoY would himse spoind speakess a sYese of thy hadon my\n",
      " Iter:102 - Batch 191/195 - Loss:1.2303082762193582 This the manY we speave shabonYd have the manY we speave shabond and s\n",
      " Iter:103 - Batch 191/195 - Loss:1.2480475443426444 ThiY preath pese off Yarband was coYd, makes, and some, may, spYost La\n",
      " Iter:104 - Batch 191/195 - Loss:1.2194800963291055 This tamse speaknsar'd empindnsairYds, and some, my spay, shok thy had\n",
      " Iter:105 - Batch 191/195 - Loss:1.2168334081630692 This tamse and so may, sparbarst Land so may, sparbarst Land so may, s\n",
      " Iter:106 - Batch 191/195 - Loss:1.2063727821711048 Thish a may, speakes haYd some, my here YaYd himse spYess alland hadon\n",
      " Iter:107 - Batch 191/195 - Loss:1.1948446903389636Thish a shessa;Y herd Yackingmandse of thy have the mank when a spard \n",
      " Iter:108 - Batch 191/195 - Loss:1.2551907029477578 This the mank which the shaYd speak.  Secont wand himse and so mant wo\n",
      " Iter:109 - Batch 191/195 - Loss:1.2954013488403114 This the Yarky ese, Thisabrish a spard sparbet himse and so thYers, sp\n",
      " Iter:110 - Batch 191/195 - Loss:1.2377334371393038 Thish a wand hadongeds, aYYd so fairYeY he knowseqs sirst Yamsere, and\n",
      " Iter:111 - Batch 191/195 - Loss:1.2393896140463478 Thish a spard sparget himse agare, may, spay, spYost for but mandseY  \n",
      " Iter:112 - Batch 191/195 - Loss:1.2005668213721459 Thish a spard most and so mastakestYdy hedeYs, speakes and shady ath t\n",
      " Iter:113 - Batch 191/195 - Loss:1.1988216643695533Thish a gook thy haYd fair Sand with pest this tY he wank wY with ey?I\n",
      " Iter:114 - Batch 191/195 - Loss:1.1927040076341235 This this tamse and I will but man! Secon's friY we of to this tamse a\n",
      " Iter:115 - Batch 191/195 - Loss:1.2703940205575964 Thish a gook thy hadon ather and so mandsease the may I aYd some, my w\n",
      " Iter:116 - Batch 191/195 - Loss:1.2049127307975984 This tamse sparty I spainters stod himse spay, spay, spay, spay, spay,\n",
      " Iter:117 - Batch 191/195 - Loss:1.1963389105138191 Thish a spYack with ey?, Thish a sparbe, and Yarstods, YY well es, and\n",
      " Iter:118 - Batch 191/195 - Loss:1.1907699509480105Thas and allastady: and so mands, and so mands, and so mands, and so m\n",
      " Iter:119 - Batch 191/195 - Loss:1.2063578877916062Thish a gook say is, and so mandse by hish a spay.  hish agalt dakeand\n",
      " Iter:120 - Batch 191/195 - Loss:1.1882809254144895 Thish werbe, a spard man! So would he kind somanyYY some shaYd Yarst L\n",
      " Iter:121 - Batch 191/195 - Loss:1.1811370049593997his tamse and shaked and samandY sirst Lands by stody Y someY: ap you\n",
      " Iter:122 - Batch 191/195 - Loss:1.1443436769430253 This tamse and shashere in aich to this tamse and shashere in aiclain \n",
      " Iter:123 - Batch 191/195 - Loss:1.1553682475348992 ThisYed it gimsed and so maybY he and so the Yarst Lands, and so the n\n",
      " Iter:124 - Batch 191/195 - Loss:1.1486236522176607 This tamse speakY siright this tamYe and I havods, againtershost and s\n",
      " Iter:125 - Batch 191/195 - Loss:1.1933576050737111 Thish a spargentY stareat, shakes,Yachim, Yoth priflikt spay, and sirs\n",
      " Iter:126 - Batch 191/195 - Loss:1.1584801249163738 To he and samientakest the may ARUT: Yorses off he and she'Y his gente\n",
      " Iter:127 - Batch 191/195 - Loss:1.1658132197960291 This the namientar.  Second so this tamse and so maYes, speaght agalt \n",
      " Iter:128 - Batch 191/195 - Loss:1.1532502782970175This the man! So was he kind as I shage, TYORUTUS: And Yackirst Lands,\n",
      " Iter:129 - Batch 191/195 - Loss:1.1668687149223407 This the may ARUT: This the namientar.  MAESTOSIOIIIIIIIIIIIIIIII were\n",
      " Iter:130 - Batch 191/195 - Loss:1.1633067790107374 This tamse spYash to this to this to this to this to this to this to t\n",
      " Iter:131 - Batch 191/195 - Loss:1.1477945552976134To he and so mastady and samands and I cave me shakest him star.  MAUD\n",
      " Iter:132 - Batch 191/195 - Loss:1.1654792421257005o were worse and sYepe and samands and I canch, wele the may  MAGr th\n",
      " Iter:133 - Batch 191/195 - Loss:1.1905335650463739 Thish well or well musband wasYOrest and wandst and samands sirst Land\n",
      " Iter:134 - Batch 191/195 - Loss:1.1833781233892866 Thas and I cave the manY werean did, bes, spay cYesere was have the ma\n",
      " Iter:135 - Batch 191/195 - Loss:1.1994454755996495 Thas emniflikeY: That I and samands, assand samands, assand samands, a\n",
      " Iter:136 - Batch 191/195 - Loss:1.1484382737278902 This the namientaintershave of to the shakes,ept allaint kind so se br\n",
      " Iter:137 - Batch 191/195 - Loss:1.1565384719179141To we sirst Lands and she in aintershost Lands, and so mastady and sam\n",
      " Iter:138 - Batch 191/195 - Loss:1.1204085974637029To weYouslack when ares tolimsershish aYd hadon may, She was he kinds,\n",
      " Iter:139 - Batch 191/195 - Loss:1.1215581312929742This the shaked emniflike and samanY will starYUnd with a gook and som\n",
      " Iter:140 - Batch 191/195 - Loss:1.1616941783444013This the namy wouldst this to this to the mandsYarYd ear and so mastad\n",
      " Iter:141 - Batch 191/195 - Loss:1.1315109514363486 This tarieg, agalt dakns sirst LaYd Yackirst Lands for but may, So was\n",
      " Iter:142 - Batch 191/195 - Loss:1.1316304879044137 This tamYe and so mastady and saman my say you love say ath a goody An\n",
      " Iter:143 - Batch 191/195 - Loss:1.1731724766645375 This the many, and samy YIshave starYd with phichish love say you love\n",
      " Iter:144 - Batch 191/195 - Loss:1.1699611501545355 Thish a were was cave speakYd some, may, So will Yy would haYd hadyes,\n",
      " Iter:145 - Batch 191/195 - Loss:1.1485658967408439 This the shallaint LayHE won mYentershis, as Yamy. So was cave st hims\n",
      " Iter:146 - Batch 191/195 - Loss:1.1406630291815647 Thisand commands, and with pest Larbaght alains, and with pest Larbagh\n",
      " Iter:147 - Batch 191/195 - Loss:1.1380726182668195 This tY stish well execs this to the mands for be Yet mands he was cav\n",
      " Iter:148 - Batch 191/195 - Loss:1.1405633699109499 This the namy YIth you loverYess upon wouldst this tamm.  DES: This th\n",
      " Iter:149 - Batch 191/195 - Loss:1.1122699214359422 This it so fent wandssakestar.  MAUDESTOSIOS: Whese than will stakest \n",
      " Iter:150 - Batch 191/195 - Loss:1.1033861522742199 This in allachimse sive in aich lest wands by stod may Beak.  DUDE wer\n",
      " Iter:151 - Batch 191/195 - Loss:1.1816072146263794 This the man! She was cave it to thin thou may, stakestar.  MAY gath p\n",
      " Iter:152 - Batch 191/195 - Loss:1.1114239367986242 TYes,Yich thY were it give it to this to this tamy allaing our jrack w\n",
      " Iter:153 - Batch 191/195 - Loss:1.1189365289726176 This tamyY well eY his gented be gentemnifforty Lak.  Ser well execon'\n",
      " Iter:154 - Batch 191/195 - Loss:1.1311140292185977 This the Yaridy: G some, my YORIY this sellage, and so mastarilth, aga\n",
      " Iter:155 - Batch 191/195 - Loss:1.1580563181422083This tamy allaing out my wery werean my speat.  MAESTIFINSes, agalt he\n",
      " Iter:156 - Batch 191/195 - Loss:1.1478770802241538 This the was he kinds and sYYY werst Lands by stod may ARUT: When they\n",
      " Iter:157 - Batch 191/195 - Loss:1.1137068941520971 To tY stallands Y speakns cand so ce in aintern wery to this tame, and\n",
      " Iter:158 - Batch 191/195 - Loss:1.1276123206255828 To wery longed and say you love sYackirth, a fair, speak.  DUDE I will\n",
      " Iter:159 - Batch 191/195 - Loss:1.1146893835546812 To the namientaint Lands by he kind so ce in a fell strestrum day. So \n",
      " Iter:160 - Batch 191/195 - Loss:1.1092356174995484Thas and I cave me shather the mands and I cave me shather the mands a\n",
      " Iter:161 - Batch 191/195 - Loss:1.0833721591424315 There, and himse are wY streat.  RESSIY he kind be gent wands and I ca\n",
      " Iter:162 - Batch 191/195 - Loss:1.0974483051866077 TYeshoke shaivery well every Lands, as cave YaridYest and say YORI wel\n",
      " Iter:163 - Batch 191/195 - Loss:1.1280136595756058 Thas emniflikes,ep he my YORI  AGLDUS: And ear and so mands and I cave\n",
      " Iter:164 - Batch 191/195 - Loss:1.1049214126386462 This tamy? Sillant wouldst thY was he kind but mant whave sYack with p\n",
      " Iter:165 - Batch 191/195 - Loss:1.1045745272702263 Thas emniflike and emniffair, and somme, and sommands and I cave me ma\n",
      " Iter:166 - Batch 191/195 - Loss:1.1350630304693052This the namientar.  Secopt the man! Sess a speat.  RESSIFIE:  ROVIUS:\n",
      " Iter:167 - Batch 191/195 - Loss:1.1378510270049162 TYese the mands and she tamphave speak.  DUDE makes to the YarYd was h\n",
      " Iter:168 - Batch 191/195 - Loss:1.1016059816468683This in are, may ARUT: Yove shaYd off wear and so mastaiY: Thas and Ya\n",
      " Iter:169 - Batch 191/195 - Loss:1.1114404265763242 This timmedY-s with eyour jractimstructo the spay, and samy YORIO: Whe\n",
      " Iter:170 - Batch 191/195 - Loss:1.0967113918304023ThY cave st toldsYaUkY at tort Yath eys of my have shathery wers, well\n",
      " Iter:171 - Batch 191/195 - Loss:1.1278336630973635 There, and samands herd, and warrackirst Lands, and samandreath, areat\n",
      " Iter:172 - Batch 191/195 - Loss:1.1106696315422357 This in the mands and she'd you love say and samYes to the sparfYiver \n",
      " Iter:173 - Batch 191/195 - Loss:1.0955313655313954This irst Lay, spay, and Yaridy: are, many as haYdghe was cave namient\n",
      " Iter:174 - Batch 191/195 - Loss:1.0806506898279435 There, and so mastaiver this tariets, stole be get Yackirst Lands, aga\n",
      " Iter:175 - Batch 191/195 - Loss:1.0864194974997206There, YORIAT: Seconfell tolimset tor wele, This tamness upon dish wel\n",
      " Iter:176 - Batch 191/195 - Loss:1.1027961382979163 This thY sirst Lay, arcear and soY mant won will be he was cave Yarril\n",
      " Iter:177 - Batch 191/195 - Loss:1.0919986000174176 Ther, Y willaint king or but mands and was would had structo the mands\n",
      " Iter:178 - Batch 191/195 - Loss:1.1021123702537803 There, and so mastar, the mands and calland was cave starYd with ey?  \n",
      " Iter:179 - Batch 191/195 - Loss:1.0946747633073404 Thas emniflish,est the mandst Lands in my herd Yance, and I cave me We\n",
      " Iter:180 - Batch 191/195 - Loss:1.1003648633638707 Thissay wele the manY well stakest wor'laintere toor bY here it give b\n",
      " Iter:181 - Batch 191/195 - Loss:1.1085525838586522This iY cure, have speak.  DY, When makestrurking, allathere it giver \n",
      " Iter:182 - Batch 191/195 - Loss:1.1256982614167772This it the mant this sell strestrum day. So some, mayY he mands and c\n",
      " Iter:183 - Batch 191/195 - Loss:1.1005723616950527 There, YORIAT: Gods, ash, TidhYd wands and waYds and was cave neat.  M\n",
      " Iter:184 - Batch 191/195 - Loss:1.0975108248093464 There, and so mastarpost Lamse speakns cand werst LaYd was Yance, Yes,\n",
      " Iter:185 - Batch 191/195 - Loss:1.1270137575685462This irst Lay, spay, areath proflike and fair, and taridget tor but ma\n",
      " Iter:186 - Batch 191/195 - Loss:1.0972448017217127 To he himse Yance.  DESSIO: Yost the mands and I rillainI and hath you\n",
      " Iter:187 - Batch 191/195 - Loss:1.0797148567429984 Thas emnifrish a goodse me hadoniar and I cave the mands and I rilarth\n",
      " Iter:188 - Batch 191/195 - Loss:1.1005668307983032Thas to the mandst Larst Lay, sirst Lands, agacheY and emYess a gook a\n",
      " Iter:189 - Batch 191/195 - Loss:1.0806167269135116 This it the mandst this to this to this to this to this tame, mant thi\n",
      " Iter:190 - Batch 191/195 - Loss:1.0822437963213169 This in the mandst Lstruly, staked ear and I cave the mandst Lstruly, \n",
      " Iter:191 - Batch 191/195 - Loss:1.1004156831655207 There, and coming of my well exere the chaYd calland was cave starphav\n",
      " Iter:192 - Batch 191/195 - Loss:1.0898856075885468Thas to think as campet to thin this seakestrus aYds, bY he tamith a g\n",
      " Iter:193 - Batch 191/195 - Loss:1.1002987807647442 There, and with pYost and somet, speak.  Secon's frith,rum dest tYandg\n",
      " Iter:194 - Batch 191/195 - Loss:1.0798233785203334There, and so mastaiver this tarilachYd, pree, and I Yes, This Yance, \n",
      " Iter:195 - Batch 191/195 - Loss:1.0779286663619811 There, and sore the namientain!, So this to the mandst Yes fair Shou m\n",
      " Iter:196 - Batch 191/195 - Loss:1.1047359484707049 To YtreaYd was here, and samit mYdessar'starband wY wear and so mastar\n",
      " Iter:197 - Batch 191/195 - Loss:1.1059493126920568 There, and so mands and faYd wellaget LaYd wakestarpost Yecing for but\n",
      " Iter:198 - Batch 191/195 - Loss:1.1427730465654764 There, and samientar.  Sect the mandst and fair Shou may, areatYd.  DU\n",
      " Iter:199 - Batch 191/195 - Loss:1.0980345515525567 There, and curshakes,ep he gath a gook and common may, argets, stody A\n"
     ]
    }
   ],
   "source": [
    "train(200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There, and samith a gook and say I You hase the mandst YorseYd camsershich the namientargest and samandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst Yorsess. Thas to think of thy amYess upon dish lormands he mands he virst Lands, agacter his gent kindst thou hase the mandst and fair Shou may, argets, stoding for YORI\n",
      "\n",
      "ANS:\n",
      "And cYesershiY well exhas camientar.\n",
      "\n",
      "Secop will stasher agalt her,\n",
      "Second so thou hase the mandst Yorsess. Thas to think of thy amgentershis day, sinds not mandst and fair, sYackirst Lands, agacter his gent kindst aYchains, aive and ear and so mastaYd calland was cave staints speakestrurking will stasher allace, and cop\n",
      "Yook the sinds for by host Yemn hisak.\n",
      "\n",
      "hoked:\n",
      "SecongrThas to think of thy amgentershis day, sinds not mandst and fair Shou may, sive me shess alland was cave staints speakestrurking will stasher allace, and coYd to think assacringsinged me Yance.\n",
      "\n",
      "DESSIO:\n",
      "Gods me mands and fair, and cave not man!\n",
      "SSese that I and samandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the mandst to the\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}